{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3a6fa0",
   "metadata": {
    "id": "9f3a6fa0"
   },
   "source": [
    "# Проекция. Франк-Вульф "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YdSIgd499i5m",
   "metadata": {
    "id": "YdSIgd499i5m"
   },
   "source": [
    "## Основная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NBXSpKQS9gUh",
   "metadata": {
    "id": "NBXSpKQS9gUh"
   },
   "source": [
    "Рассмотрим задачу минимизации эмпирического риска:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{x \\in \\mathcal{X} \\subset \\mathbb{R}^d} \\left[ f(x) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell_x(a_i, b_i)\\right],\n",
    "\\end{equation}\n",
    "\n",
    "где:\n",
    "- $\\ell_x(a_i, b_i)$ — функция потерь (cross-entropy loss),\n",
    "- $x$ — вектор параметров модели,\n",
    "- $\\{a_i, b_i\\}_{i=1}^n$ — выборка данных.\n",
    "\n",
    "Функция потерь для каждого объекта $i$ записывается как:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_x(a_i, b_i) = -b_i \\ln(p(x^Ta_i)) - (1 - b_i) \\ln(1 - p(x^Ta_i)),\n",
    "\\end{equation}\n",
    "\n",
    "где $p(x^Ta_i)$ — это вероятность, вычисляемая с помощью логистической функции в комбинации с линейной моделью:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x^Ta_i) = \\frac{1}{1 + \\exp(-x^T a_i)}.\n",
    "\\end{equation}\n",
    "\n",
    "Градиент для нашей целевой функции:\n",
    "\\begin{equation}\n",
    "\\nabla f(x) = \\frac{1}{n} \\sum_{i=1}^n (p(x^Ta_i) - b_i) a_i\n",
    "\\end{equation}\n",
    "В качестве константы Липшица нашей целевой функции можно брать\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{4n} \\lambda_{\\max}(A A^T)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1d30f",
   "metadata": {
    "id": "60b1d30f"
   },
   "source": [
    "К заданию приложен датасет _mushrooms_. С помощью следующего кода сформируем матрицу $A$ и вектор $b$, в которой и будет храниться выборка $\\{a_i, b_i\\}_{i=1}^n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a13738f",
   "metadata": {
    "id": "2a13738f"
   },
   "outputs": [],
   "source": [
    "#файл должен лежать в той же деректории, что и notebook\n",
    "dataset = \"mushrooms.txt\"\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "A, b = data[0].toarray(), data[1]\n",
    "\n",
    "b = b-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa919256",
   "metadata": {
    "id": "aa919256"
   },
   "source": [
    "Разделим данные на две части: обучающую и тестовую.\n",
    "\n",
    "__Важно:__ обязательно дальше при решении задания для обучения используйте train выборку, а для проверки test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e21a16fb",
   "metadata": {
    "id": "e21a16fb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966431ad",
   "metadata": {
    "id": "966431ad"
   },
   "source": [
    "Зафиксируем seed для воспроизводимости. Для генерации случайных точек используйте созданный генератор `rng` ([документация](https://numpy.org/doc/stable/reference/random/generator.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d0a39c",
   "metadata": {
    "id": "86d0a39c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cb02d",
   "metadata": {
    "id": "6b1cb02d"
   },
   "source": [
    "Для обучающей части $A_{train}$, $b_{train}$ оцените константу $L$.\n",
    "\n",
    "Можно просто вставить реализацию из прошлого дз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d887e6c",
   "metadata": {
    "id": "7d887e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = 2.5846388628723007\n",
      "mu = 0.0025846388628723007\n"
     ]
    }
   ],
   "source": [
    "# Ваше решение\n",
    "n = A_train.shape[0]\n",
    "\n",
    "eigvals = np.linalg.eigvals(A_train.T @ (A_train)) \n",
    "lambda_max = (np.max(eigvals) / (4 * n)).real\n",
    "L = lambda_max # с точностью до 1/1000 \n",
    "lmbd = L / 1000\n",
    "mu = lmbd\n",
    "print(f\"L = {L}\")\n",
    "print(f\"mu = {mu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L9ZqQxwWDfe5",
   "metadata": {
    "id": "L9ZqQxwWDfe5"
   },
   "source": [
    "Реализуйте в коде подсчет значения и градиента для нашей целевой функции. При этом $A$, $b$, необходимо подавать в качестве параметра, чтобы была возможность их менять.\n",
    "\n",
    "Необходимо использовать только библиотеку ``numpy``.\n",
    "\n",
    "Можно просто вставить немного модифицированную реализацию из прошлого дз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Q-bmNd3cDfGf",
   "metadata": {
    "id": "Q-bmNd3cDfGf"
   },
   "outputs": [],
   "source": [
    "# Ваше решение\n",
    "def p_func(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def f_func(x, A, b, lam=lmbd, eps=1e-10):\n",
    "    n = A.shape[0]\n",
    "    p = p_func(A @ x)\n",
    "    return -np.mean(b * np.log(p + eps) + (1 - b) * np.log(1 - p + eps))\n",
    "\n",
    "def grad_func(x, A, b, lam=lmbd, eps=1e-10):\n",
    "    n = A.shape[0]\n",
    "    p = p_func(A @ x)\n",
    "    return (1 / n) * A.T.dot(p - b)\n",
    "\n",
    "def criterion(x, A, b, lam=lmbd):\n",
    "    return np.linalg.norm(grad_func(x, A, b, lam)) / np.linalg.norm(grad_func(x_0, A, b, lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58557949",
   "metadata": {
    "id": "58557949"
   },
   "source": [
    "__Задача 1. (всего 5 баллов)__ Так как мы теперь решаем задачу оптимизации на шаре, необходимы методы, учитывающие это."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2377d566",
   "metadata": {
    "id": "2377d566"
   },
   "source": [
    "__а). (2 балла)__ Докажите, что для $\\ell_1$-шара с центром в $0$ и радиусом $R$  выражение для решения задачи линейной оптимизации при заданном векторе $g \\in \\mathbb{R}^d$:\n",
    "$$\n",
    "s^* = \\arg \\min_{s \\in \\mathcal{X}} \\langle s, g \\rangle.\n",
    "$$\n",
    "может быть выписано так:\n",
    "$$\n",
    "s^*: \\begin{cases}\n",
    "s^*_j = -R \\cdot \\mathrm{sign}(g_j), &j = \\min\\{\\arg\\max\\limits_{i}|g_i|\\}, \\\\ s^*_k = 0, &k \\neq j \\end{cases}\n",
    "$$\n",
    "\n",
    "Формально обоснуйте свой ответ, например, можно (необязательно именно так) использовать условия ККТ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec857ba1",
   "metadata": {
    "id": "ec857ba1"
   },
   "source": [
    "**Ваше решение**\n",
    "\n",
    "Целевая функция является линейной, поэтому её минимум на шаре достигается в одной из точек, у которых одна координата равна $\\pm R$, а остальные 0. Чтобы минимизировать выражение, найдём координату $g$ с максимальным по модулю значением ($\\arg\\max\\limits_{i}|g_i|$), и возьмём $s^*_j = -R \\cdot \\mathrm{sign}(g_j)$ (остальные индексы равны 0), ведь это и даст минимальное значение. В определении $j$ стоит `min` для учёта случая, когда индексов с максимальным по модулю значением несколько.\n",
    "\n",
    "Для проверки и обоснования выпишем лагранжиан:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(s,\\lambda) = \\sum_{i=1}^{d} s_i g_i + \\lambda \\left( \\sum_{i=1}^{d} |s_i| - R \\right)\n",
    "$$\n",
    "\n",
    "Условия ККТ в порядке 2-1-3:\n",
    "\n",
    "Доп. нежёсткость выполняется, т.к мы выбрали такую точку, что при любом $\\lambda$ второе слагаемое лагранжиана равно 0\n",
    "\n",
    "Условие стационарности можно записать в виде равенства 0 производной: $g_i + \\lambda \\cdot sign(s_i) = 0$ для любого $i$. Для того индекса $j$, который нашли из $argmax$, получим отсюда $\\lambda = |g_j|$. Для остальных мы взяли $s_i = 0$, то есть функция недифф., поэтому необходимо, чтобы $g_i$ попал в субдифф. функции $|s_i|$, равный отрезку от $-\\lambda$ до $\\lambda$. Поскольку $\\lambda$ - максимальное по модулю значение $g_i$, все остальные координаты $g$ попадут в этот отрезок.\n",
    "\n",
    "Как было найдено, $\\lambda = |g_j| > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f02cc",
   "metadata": {
    "id": "6e3f02cc"
   },
   "source": [
    "\n",
    "__б). (1 балл)__ Реализуйте отдельно решение задачи линейной оптимизации из предыдущего пункта (радиус шара $R$ лучше передавать в качестве параметра). Реализуйте метод Франк-Вульфа для нашей задачи.\n",
    "\n",
    "**Псевдокод алгоритма**\n",
    "\n",
    "*Инициализация:*\n",
    "\n",
    "- Величина шага $\\left\\{ \\gamma_k = \\frac{2}{k+2} \\right\\} _{k=0}$\n",
    "- Стартовая точка $ x^0 \\in \\mathbb{R}^d $\n",
    "- Количество итераций $ K $\n",
    "\n",
    "*$k$-ая итерация:*\n",
    "1. Подсчитать направление спуска:  \n",
    "   $$ \\nabla f(x^k) $$\n",
    "   \n",
    "2. Найти оптимальное направление:  \n",
    "   $$ s^k = \\arg \\min_{s \\in \\mathcal{X}} \\langle s, \\nabla f(x^k) \\rangle $$\n",
    "\n",
    "3. Обновить значение:  \n",
    "   $$ x^{k+1} = (1 - \\gamma_k) x^k + \\gamma_k s^k $$\n",
    "\n",
    "Используйте предложенную функцию для реализации алгоритма и допишите недостающие фрагменты. После чего для проверки правильности загрузите функцию в [контест TBD](https://contest.yandex.ru/contest/66540/enter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "671dfd0c",
   "metadata": {
    "id": "671dfd0c"
   },
   "outputs": [],
   "source": [
    "# Ваше решение\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "def FrankWolfe(grad, criterion, gamma, x_0, A, b, eps, max_iter):\n",
    "    # сил больше нет\n",
    "\n",
    "    '''\n",
    "       grad(x) - функция, которая считает градиент целевой функции;\n",
    "       criterion(x) - функция, которая считает критерий;\n",
    "       x_0 - начальная точка;\n",
    "       eps - точность сходимости (обычно 1e-8);\n",
    "       max_iter - количество итераций.\n",
    "    '''\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    x_k = np.copy(x_0)\n",
    "    err_x_0 = criterion(x_k, A, b)\n",
    "    errors.append(criterion(x_k, A, b) / err_x_0)\n",
    "    for k in trange(max_iter):\n",
    "\n",
    "        # your code\n",
    "        g = grad(x_k, A, b)\n",
    "        s_k = np.zeros_like(x_k)\n",
    "        j = np.argmax(np.abs(g))\n",
    "        s_k[j] = 1\n",
    "        x_k = x_k * (1 - gamma) + s_k * gamma\n",
    "        \n",
    "        errors.append(criterion(x_k, A, b) / err_x_0)\n",
    "        if errors[-1] < eps:\n",
    "            break\n",
    "            \n",
    "    return x_k, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d708445",
   "metadata": {
    "id": "2d708445"
   },
   "source": [
    "__в). (2 балла)__ Решите задачу оптимизации на обучающей выборке с помощью реализованного метода. Возьмите $R = 5$ и стартовую точку в $0$. В качестве критерия используйте следующее выражение:\n",
    "$$\n",
    "\\text{gap}(w^k) = \\max_{y \\in \\mathcal{X}} \\langle \\nabla f(w^k), w^k - y \\rangle,\n",
    "$$\n",
    "и усредненную версию $\\frac{1}{k} \\sum_{i=1}^k \\text{gap}(w^i)$. Такой критерий используем, так как не знаем значение $f^*$ и не можем гарантировать, что $\\nabla f(w^*) = 0$. Но можно показать, что $\\text{gap}(w^k) \\geq f(w^k) - f^*$, а также доаказать сходимость метода Франк-Вульфа по такому критерию, а значит сходимость по $\\text{gap}(w^k)$ и дает хорошее понимание о поведении $f(w^k) - f^*$.\n",
    "\n",
    "Постройте два графика сходимости: значение критерия сходимости (две версии, обычная и усреднённая) от номера итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee49367",
   "metadata": {
    "id": "3ee49367"
   },
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0dbc13",
   "metadata": {
    "id": "0d0dbc13"
   },
   "source": [
    "Выведите решение, полученное с помощью метода Франк-Вульфа. Что необычного увидели? Возможно, какое-то значение часто повторяется? Объясните, почему так получается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e34912",
   "metadata": {
    "id": "78e34912"
   },
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9efce",
   "metadata": {
    "id": "5ee9efce"
   },
   "source": [
    "__Ваше решение__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838ef00",
   "metadata": {
    "id": "6838ef00"
   },
   "source": [
    "### Дополнительная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65373c6b",
   "metadata": {
    "id": "65373c6b"
   },
   "source": [
    "__Задача 1. (всего 2 балла)__\n",
    "\n",
    "В прошлом задании мы, используя полученное решения задачи оптимизации, предсказывали ответы на тестовой выборке. Напомним суть: исходная задача регрессии является задачой машинного обучения и с помощью линейной модели $g$ можно предсказывать значения меток $y$. Пусть у нас есть сэмпл $x_i$, ответ модели для этого сэмпла есть $g(w^*, x^i)$. Тогда предсказывающее правило можно сформулировать следующим довольно естественным образом:\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "1, & g(w^*, x^i) \\geq 0,\n",
    "\\\\\n",
    "0, & g(w^*, x^i) < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "Cделав предсказания на тестовой выборке $X_{test}$, можно сравните результат с реальными метками $y_{test}$. Количество правильно угаданных меток есть точность/accuracy модели.\n",
    "\n",
    "Посмотрите какую дает модель обученная с помощью метода Франк-Вульфа. Варьируйте $R = 5, 10, 20, 50, 100, 1000$. Постройте три графика:\n",
    "1) точность итоговой модели от $R$,\n",
    "2) количество ненулевых компонент в итоговом решении метода Франк-Вульфа от $R$,\n",
    "3) точность от количества ненулевых компонент в итоговом решении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fa679",
   "metadata": {
    "id": "c95fa679"
   },
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147d812",
   "metadata": {
    "id": "e147d812"
   },
   "source": [
    "Сделайте вывод. В первую очередь обратите внимание на точки максимума или минимума."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa7b8d",
   "metadata": {
    "id": "61aa7b8d"
   },
   "source": [
    "__Ваше решение__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db33a9",
   "metadata": {
    "id": "64db33a9"
   },
   "source": [
    "__Задача 2. (всего 3 балла)__\n",
    "\n",
    "Нашу задачу можнно решать и с помощью метода градиентного спуска с евклидовой проекцией. Для этого нужно уметь делать проекцию на $\\ell_1$-шар, поэтому нужно вывести ее ручками."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfe1af",
   "metadata": {},
   "source": [
    "__а). (1 балл)__ \n",
    "Рассмотрим задачу поиска проекции на $\\ell_{1}$-шар радиуса $R$ с центром в 0:\n",
    "$$\n",
    "    \\arg\\min\\limits_{|y|_1 \\leq R} \\left[ \\frac{1}{2} | x - y |^{2}_{2} \\right].\n",
    "$$\n",
    "\n",
    "Используя одно из необходимых условий ККТ, покажите, что оптимальное решение можно найти как\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Pi_{\\mathcal{X}} (x) =\n",
    "    \\begin{cases}\n",
    "        x, & \\text{если}\\ \\| x \\|_{1} \\leq R; \\\\\n",
    "        \\text{sign}(x_{i})(|x_{i}| - \\lambda)_{+}\\ \\forall i \\in \\overline{1, d}, & \\text{если}\\ \\| x \\|_{1} > R,\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "где $\\lambda$ - решение двойственной задачи. Распишите достаточные условия ККТ и составьте уравнение, из которого можно найти $\\lambda$.\n",
    "\n",
    "*Hint: если записать кружочек в группу, то мы Вам дадим подсказку где найти решение)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec2d9e",
   "metadata": {
    "id": "a8ec2d9e"
   },
   "source": [
    "__Ваше решение__\n",
    "\n",
    "*Hint: Воспользуйтесь теоремой Каруша-Куна-Такера и запишите Лагранжиан*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf3acf",
   "metadata": {
    "id": "55bf3acf"
   },
   "outputs": [],
   "source": [
    "# Ваше решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fb696",
   "metadata": {
    "id": "758fb696"
   },
   "source": [
    "__б). (2 балла)__ \n",
    "Решите задачу оптимизации на обучающей выборке с помощью градиентного спуска с евклидовой проекцией. (Код градиентного спуска можете взять из первой прак дз)\n",
    "\n",
    "Сравните на графиках сходимость градиентного спуска и метода Франк-Вульфа: 1) значение критерия от номера итерации, 2) значение критерия от времени. Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9979caf",
   "metadata": {
    "id": "c9979caf"
   },
   "outputs": [],
   "source": [
    "#ваше решение (Code и Markdown)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "iitp",
   "language": "python",
   "name": "iitp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
